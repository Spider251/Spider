day02回顾
1. 正则解析
    1. 分组(想要抓取什么内容就加小括号())
2. 数据抓取步骤
    1. 找URL
    2. 写正则表达式
    3. 定义类, 写程序框架
    3. 补全代码
3. 存入CSV文件
    import csv
    with open("test.csv","a",newline="") as f:
        writer = csv.writer(f)
        writer.writerow([列表])
4. 存入MySQL数据库
    1. db = pymysql.connect("127.0.0.1","root","123456","库名",3306)
    2. cursor = db.cursor()
    3. cursor.execute('sql命令',[列表补位])
    4. db.commit()
    5. cursor.close()
    6. db.close()
5. mongodb流程
    1. conn = pymongo.MongoClient("",27017)
    2. db = conn["库名"]
    3. myset = db["集合名"]
    4. myset.insert_one({字典})
    终端操作
    1. show dbs
    2. use 库名
    3. show collections
    4. db.集合名.find().pretty()
    5. db.集合名.count()
    6. db.dropDatabase()
6. 远程存入MySQL
    1. 开启远程连接 : # bind-address=127.0.0.1
    2. 添加授权用户
        mysql>grant all privileges on *.* to "root"@"%" indentified by "123456" with grant option;
    3. 设置防火墙
       方法1 : 允许外部访问本机的3306端口 : sudo ufw allow 3306
       方法2 : 直接关闭防火墙 : sudo ufw disable
7. Anaconda Prompt安装模块
    1. 右键, 以管理员身份去打开 Anaconda Prompt
    2. 在(base)C:\Users\Administrator\执行
        conda install 模块名
*********************************************************************************************************
Day03笔记
1. Cookie模拟登陆
    1. 什么是cookie session
        HTTP协议是一种无连接协议, 客户端和服务器交互仅仅局限于请求/响应之间, 
        下一次再请求时, 服务器会认为是一个新的客户端, 为了维护他们之间的连接, 
        让服务器知道是上一个用户发起的请求, 必须在一个地方保存客户端的信息
        cookie : 客户端信息确定用户身份
        session : 服务端信息确定用户身份
    2. 使用cookie模拟登陆人人网
        1. 先登陆成功次,1 获取到cookie
        2. 拿着cookie去抓取需要登陆才能看到的页面
2. requests模块
    1. 安装 : 以管理员身份去打开Anaconda Prompt conda install requests
    2. 常用方法
        1. get(url,headers=headers)
        2. 响应对象res属性
            1. encoding : 响应编码
                res.encoding = "utf-8"
            2. text : 字符串
            3. content : bytes
            4. status_code : HTTP响应码
            5. url : 返回实际数据的URL地址
        3. 三步走
            1. res = requests.get(url,headers...)
            2. res.encoding = "utf-8"
            3. html = res.text  
        4. 非结构化数据保存(图片)
            html = res.content
            with open("***.jpg","wb") as f:
                f.write(html)  
3. get()方法中参数
    1. 查询参数(params) : 字典
    res = requests.get(baseurl,params={},headers=headers)
    * 自动对params字典进行编码, 自动拼接URL地址
    输入搜索内容, 再输入第几页,
    2. 使用代理(proxies)
        1. 获取IP地址的网站
            快代理
            全网代理
        2. 普通代理
            1. 格式 proxies={"协议":"协议://IP:端口号 "}
        3. 私密代理
            1. 格式
                proxies={"协议"}
        4. web客户端验证(auth)
            1. auth = ("用户名","密码")
                auth = ("tarenacode","code_2013")
        5. SSL证书认证(verify)
            1. verify = True(默认) : 进行CA证书认证
            2. verify = False     : 不进行认证    
            ### 参数为True, 去访问https网站(没有进行CA认证),抛出异常: SSLError
        6. get(url,params,headers,proxies,auth,verify)
4. post()方法
    1. requests.post(url,data=data,...)
    2. data : 字典, Form表单数据, 不用编码, 不用转码
***********************************************************************************
5. xpath工具(解析)
    1. 在XML文档中查找信息的语言,同样适用于HTML文档的检索
    2. xpath辅助工具
        1. Chrome插件 : XPath Helper
            打开/关闭 : Ctrl + shift + x
        2. Firefox插件: XPath Checker
        3. XPath编辑工具 : XML quire
    3. 匹配演示
        1. 查找所有的book节点
        2. 查找所有book节点下的title节点宏, lang属性为"en"的节点
            //book/title[@lang="en"]
        3. 查找bookstore下的第二个book节点下面的title子节点
            //bookstore/book[2]/title
        4. 选取节点
            // : 从整个文档中查找节点
                //price  //book//price
            @ : 选取某个节点的属性值
                //title[@lang"en"]
        5. @的使用
            1. 选取节点1类节点 : //title[@lang][@class="en"]
        6.匹配多路径 
            1. xpath 表达式1 | xpath表达式2
            2. 获取bookIDE节点下的title指节垫和price子节点
        7. 函数
            1. contains() : 匹配一个属性值中包含某些字符串的节点
                //title[contains(@lang,"e")]
            2. text() : 获取文本
                //title[contains(@lang,"e")]/text()
6. lxml库及xpath使用
    1. 安装
        管理员Prompt : conda install lxml
    2. 使用流程
        1. 导模块
            from lxml import etree
        2. 创建解析对象
            parseHtml = etree.HTML(html)
        3. 调用xpath
            rList = parseHtml.xpath('xpath表达式')
        ### 只要调用了xpath,结果一定是一个列表 ###
    
    
    

    
    
    
    
    
    
    
    